\documentclass[12pt]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{secdot}

\textheight=24cm
\textwidth=16cm
\oddsidemargin=0pt
\topmargin=-1.5cm
\parindent=24pt
\parskip=0pt
\tolerance=2000\flushbottom

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[]{algorithm2e}
\usepackage[shortlabels]{enumitem}
\theoremstyle{definition}
\usepackage{float}
\usepackage{graphicx}

\date{}
\author{Константин Чернис, группа 694}

\newtheorem*{Def*}{Определение}
\newtheorem*{Th*}{Теорема}

\newtheorem{Def}{Определение}
\newtheorem{Th}{Теорема}
\newtheorem{Prop}{Предложение}
\newtheorem{St}{Утверждение}
\newtheorem{Cor}{Следствие}
\numberwithin{Def}{section}
\numberwithin{Th}{section}
\numberwithin{Prop}{section}
\numberwithin{St}{section}
\numberwithin{Cor}{section}

\newenvironment{Proof}                            {\par\noindent{\bf Доказательство.}}        {\hfill$\scriptstyle\blacksquare$}

\newcommand{\Set}[2]{
  \{\, #1 \mid #2 \, \}
}
\makeatletter
\renewcommand{\@algocf@capt@plain}{above}% formerly {bottom}s
\makeatother

\begin{document}
\begin{titlepage}
	\centering
	{\scshape\Large Методы оптимизации\par}
	\vspace{1.5cm}
	{\huge\bfseries Regularized Nonlinear Acceleration\par}
	\vspace{2cm}
	{\Large Чернис Константин, группа 694\par}
	\newpage
\end{titlepage}
\tableofcontents
\newpage

В данном проекте представлен алгоритм нелинейного ускорения итерационных методов.
Также теоретически и экспериментально исследовано влияние алгоритма на скорость
сходимости различных методов первого и второго порядков.

Для начала рассмотрим более простой алгоритм ускорения, а именно, ускорение 
Андерсона.

\section{Anderson acceleration}

Пусть необходимо решить задачу оптимизации:
\begin{align*}
	\min_{x\in \mathbb{R}^n} f(x),
\end{align*}
где $f(x)$ сильно выпукла с константой $\mu$, а её градиент липшецев с константой
$L$. Будем искать решение с помощью метода неподвижной точки:
\begin{equation}\label{FPI}
x_{i+1}=g(x_i),\quad i=\overline{0,k}
\end{equation}

Пусть $g(x)$ дифференцируема и $G$~"--- матрица Якоби функции $g$ в точке $x^*$.
Далее считаем $G$ симметричной положительно определённой матрицей,
$G\preceq\sigma I$, где $\sigma<1$. Тогда из \eqref{FPI} получаем
линейный метод неподвижной точки:
\begin{equation}\label{LFPI}
x_{i+1}=g(x^*)+G(x_i-x^*)+O\left(\|x_i-x^*\|^2_2\right),\quad i=\overline{1,n}
\end{equation}
Пренебрегая вторым порядком малости и учитывая, что $x^*$~"--- неподвижная точка
$g(x)$, то есть $g(x^*)=x^*$, получаем
$$
x_{i+1}-x^*=G(x_i-x^*)$$
В силу того, что $\|G\|_2\leqslant\sigma< 1$, получаем линейную скорость
сходимости:
\newcommand{\euc}[1]{\|#1\|_2}
$$
\euc{x_i-x^*}\leqslant \sigma\euc{x_{i-1}-x^*}\leqslant \sigma^i\euc{x_0-x^*}
$$

Рассмотрим линейную комбинацию $x_i$ после $k$ итераций:
$$
\sum_{i=0}^k c_ix_i=\sum_{i=0}^k c_ix^*+\sum_{i=0}^k c_iG(x_i-x^*)=
\left(\sum_{i=0}^k c_i\right)x^*+\left(\sum_{i=0}^k c_i G^i\right)(x_0-x^*)
$$
и определим многочлен
$$
p(z):=\sum_{i=0}^k c_iz^i
$$
Теперь линейную комбинацию можно записать с помощью матричного многочлена $p(G)$,
добавив ограничение $p(1)=\sum_{i=0}^kc_i=1$:
\begin{equation}\label{error_term}
\sum_{i=0}^kc_ix_i=x^*+\underbrace{p(G)(x_0-x^*)}_{\text{ошибка}}
\end{equation}
Будем искать коэффициенты $c$ (или $p$ соответственно), которые минимизируют
ошибку:
$$
c^\star=\underset{\{c\in\mathbb{R}^{k+1}:c^\mathsf{T}\mathbf{1}=1\}}
{\arg\min}\left\|\sum_{i=0}^kc_iG^i(x_0-x^*)\right\|_2=
\underset{\{p\in\mathbb{R}_k[x]:p(1)=1\}}
{\arg\min}\left\|p(G)(x_0-x^*)\right\|_2
$$
где $\mathbb{R}_k[x]$~"--- пространство многочленов степени не выше $k$.

Следующее предложение даёт оценку сверху на размер ошибки:

\begin{Prop}\label{prop_bound}
	Пусть
	\begin{enumerate}
		\item последовательность $x_i,\,i=\overline{0,n}$, получена из
		\eqref{LFPI};
		\item $G$~"--- симметричная матрица Якоби $g$, для которой
		выполнено ${0\preceq G\preceq\sigma I}$, $\sigma<1$;
		\item $x^*$~"--- неподвижная точка $g$.
	\end{enumerate}
	Тогда $l_2$ норма ошибки \eqref{error_term} ограничена:
	\begin{equation}
		\left\|\sum_{i=0}^kc_i^\star x_i-x^*\right\|_2\leqslant\begin{cases}
		\dfrac{2\beta^k}{1+\beta^{2k}}\|x_0-x^*\|_2,&\text{если }k<m \\
		0&\text{иначе,}
	\end{cases}
	\end{equation}
	где $m$~"--- число различных собственных значений $G$ и
	$$
	\beta=\dfrac{1-\sqrt{1-\sigma}}{1+\sqrt{1-\sigma}}<1
	$$
\end{Prop}

Данная оценка получена из полу-итерационного метода Чебышёва,
после преобразования получаем
$$
\left\|\sum_{i=0}^kc_i^\star x_i-x^*\right\|_2\leqslant
\left(1-\sqrt{1-\sigma}\right)^k\euc{x_0-x^*}\ll\sigma^k\euc{x_0-x^*},
$$
так что достигнуто ускорение. В то же время, для применения этого метода
требуется знание оценки $\sigma$ матрицы Якоби $G$ в точке $x^*$, кроме того,
коэффициенты в линейной комбинации могут быть очень большими, что влияет
на численную стабильность алгоритма.

В связи с перечисленными выше проблемами далее сосредоточимся на методе, который
будет приближённо минимизировать ошибку \eqref{error_term}. В силу того, что $G$ 
и $x^*$ неизвестны, будем работать с невязкой:
$$
r_i=x_{i+1}-x_i=g(x_i)-x_i,
$$
тогда линейный метод неподвижной точки \eqref{LFPI} принимает вид
$$
r_i=x_{i+1}-x_i=(G-I)(x_i-x^*),
$$
а линейная комбинация записывается как
$$
\sum_{i=0}^kc_ir_i=(G-I)\sum_{i=0}^k c_i(x_i-x^*)=(G-I)p(G)(x_0-x^*),
$$
что равно ошибке $p(G)(x_0-x^*)$, умноженной на $(G-I)$, то есть использование
этих коэффициентов будет приближённо минимизировать ошибку.

\begin{Prop}\label{prop_sigma}
	Пусть
	$$
	c^\star=\underset{\{c\in\mathbb{R}^{k+1}:c^\mathsf{T}\mathbf{1}=1\}}
	{\arg\min}\left\|\sum_{i=0}^kc_ir_i\right\|_2
	$$
	Тогда последовательность $x_i,\,i=\overline{0,k}$, усреднённая с помощью
	коэффициентов $c^*$, удовлетворяет соотношению
	\begin{equation}\label{approx_bound}
	\left\|\sum_{i=0}^kc_i^*x_i-x^*\right\|_2\leqslant\dfrac1{1-\sigma}
	\underset{\{c\in\mathbb{R}^{k+1}:c^\mathsf{T}\mathbf{1}=1\}}
	{\arg\min}\left\|\sum_{i=0}^kc_iG^i(x_0-x^*)\right\|_2,
	\end{equation}
	где $0\preceq G\preceq\sigma I,\,\sigma<1$.
\end{Prop}

Отсюда получаем ускорение Андерсона:
\\\\
\begin{algorithm*}[H]
\caption{Anderson acceleration}
\LinesNumbered
 \KwData{Последовательность $x_0,x_1,\ldots,x_{k+1}\in\mathbb{R}^d$.}
 Составить матрицу $R=[r_0,\ldots,r_k]$\;
 Решить задачу
 $$
 c^*=\underset{\{c\in\mathbb{R}^{k+1}:c^\mathsf{T}\mathbf{1}=1\}}
	{\arg\min}\left\|\sum_{i=0}^kc_ir_i\right\|_2
 $$
 
 
 \KwResult{Аппроксимация $\widehat{x^*}=\sum_{i=0}^kc_i^*x_i$, 
 удовлетворяющая \eqref{approx_bound}.}
\end{algorithm*}

\paragraph{}
Из ККТ можно вывести, что решение получается в 2 шага:
\begin{enumerate}
	\item Решить $R^\mathsf{T}Rz=\mathbf{1}$
	\item $c^*=z/(\mathbf{1}^\mathsf{T}z)$
\end{enumerate}

\section{Regularized nonlinear acceleration}

Чтобы повысить численную стабильность алгоритма и обеспечить его работу, если
функция $g$ содержит шум вида $x_{i+1}-x_i=g(x_i)-x_i=G(x_i-x^*)+e_i$,
добавим регуляризацию в предложенный выше алгоритм:
\\\\
\begin{algorithm*}[H]
\caption{Regularized Nonlinear Acceleration (RNA)}
\LinesNumbered
 \KwData{Последовательность $x_0,x_1,\ldots,x_{k+1}\in\mathbb{R}^d$,
 полученная из метода неподвижной точки, и регуляризационный параметр
 $\lambda>0$.}
 Составить матрицу $R=[r_0,\ldots,r_k]$, где $r_i=x_{i+1}-x_i$\;
 Решить задачу
 $$
 c^*=\underset{c^\mathsf{T}\mathbf{1}=1}{\arg\min}\euc{Rc}^2+\lambda\euc{c}^2,
 $$
 или, что эквивалентно, решить $(R^\mathsf{T}R+\lambda I)z=\mathbf{1}$ и взять
 $c^*_\lambda=z/\mathbf{1}^\mathsf{T}z$.
 
 \KwResult{Аппроксимация $\widehat{x^*}=\sum_{i=0}^k(c^*_\lambda)_ix_i$, 
 удовлетворяющая \eqref{approx_bound}.}
\end{algorithm*}

\paragraph{}Наконец, добавим поиск по сетке для выбора $\lambda$ и backtracking
для выбора размера шага:
$$
\min_{t>0}f(x_0+t(x_{extr}(\lambda)-x_0))
$$
\\\\
\begin{algorithm*}[H]
\caption{Adaptive Regularized Nonlinear Acceleration (ARNA)}
\LinesNumbered
 \KwData{Последовательность $x_0,x_1,\ldots,x_{k+1}\in\mathbb{R}^d$,
 полученная из метода неподвижной точки, границы
 $[\lambda_{\min},\lambda_{\max}]$ и целевая функция $f(x)$.}
 Разбить отрезок $[\lambda_{\min},\lambda_{\max}]$ на $k$ частей, используя
 логарифмический масштаб\;
 Составить матрицу $R=[r_0,\ldots,r_k]$, где $r_i=x_{i+1}-x_i$\;
 Построить матрицу $M=R^\mathsf{T}R/\euc{R^\mathsf{T}R}$\;
 \For{$j=\overline{1,k}$}{
 	Решить систему $(M+\lambda_j)z=\mathbf{1}$\;
 	Нормировать решение: $c^*_{\lambda_j}=z/\mathbf{1}^\mathsf{T}z$\;
 	Вычислить $x_{extr}(\lambda_j)=\sum_{i=0}^k(c^*_{\lambda_j})_ix_i$\;
 }
 Выбрать $x^*_{extr}=\arg\min_{j=\overline{1,k}}
 f(x_{extr}(\lambda_j))$\;
 Задать $F_t=f(x_0+t(x^*_{extr}-x_0))$\;
 $t:=1$\;
 \While{$F_{2t}<F_t$}{
 $t=2t$\;
 }
 \KwResult{Аппроксимация $(x_0+t(x^*_{extr}-x_0))$}
\end{algorithm*}

\section{Численные эксперименты}

\subsection{Ускорение градиентного спуска для квадратичной функции}

\def\tr{\mathsf{T}}
Рассмотрим квадратичную функцию $f=\dfrac 12x^\mathsf{T}Ax-b^\tr x$, тогда её
градиент имеет вид
$$
\nabla f(x)=Ax-b=A(x-x^*),
$$
где последний переход верен в силу $Ax^*=b$. Пусть
$\mu I\preceq A\preceq \sigma I$, тогда градиентный спуск с шагом $1/L$ 
записывается как
$$
x_{i+1}=x_i-\dfrac 1L\nabla f(x_i)=x_i-\dfrac 1L A(x_i-x^*)=
\underbrace{(I-A/L)}_G(x_i-x^*)+x^*,
$$
откуда $0\preceq G\preceq \left(1-\dfrac \mu L\right)I$

Используя Предложение \ref{prop_bound} и Предложение \ref{prop_sigma}, получаем
следующую оценку на скорость сходимости градиентного спуска, ускоренного
алгоритмом Андерсона:
$$
\left\|\sum_{i=0}^Nc_i^*x_i-x^*\right\|_2\leqslant\dfrac L\mu
\dfrac{2\beta^k}{1+2\beta^k}\euc{x_0-x^*},\quad\text{ где }
\beta=\dfrac{1-\sqrt{\dfrac \mu L}}{1+\sqrt{\dfrac \mu L}}
$$

Преобразовывая, получаем
$$
\left\|\sum_{i=0}^Nc_i^*x_i-x^*\right\|_2\lessapprox
\dfrac L\mu\cdot 2\beta^k\euc{x_0-x^*},
$$
что соответствует оценке сходимости оптимального метода Нестерова 
\ref{Nesterov}. Продемонстрируем скорость сходимости на практике:
\begin{figure}[H]
  \includegraphics[width=\linewidth]{quadratic_function.png}
\end{figure}
Более медленную сходимость ARNA по сравнению с Anderson на начальном этапе
можно объяснить неточными значениями коэффициентов из-за присутствия
регуляризации. В остальном ARNA демонстрирует отличную скорость сходимости
(для Anderson \textit{choose\_best} не применялся в силу того, что эта опция
не исправляет застревание в этом случае).

\subsection{Ускорение Ridge регрессии}

Применим ускорение к оптимизации Ridge регрессии для стандартного датасета
boston. Функция потерь задаётся как
$$
f(w, \lambda)=\sum_{i=1}^m (Z^\mathsf{T}w-y)^2+\dfrac\lambda2 \euc{w}^2,
$$
она строго выпукла с $\mu=\lambda$, а её градиент липшецев с константой
$L=\euc{2Z^\mathsf{T}Z+\lambda I}$. Снова получаем сходимость, аналогичную
методу Нестерова:
\begin{figure}[H]
  \includegraphics[width=\linewidth]{ridge.png}
\end{figure}
Заметим, что хоть на начальном этапе ускоренные алгоритмы и вырываются вперёд,
в дальнейшем характер сходимости почти не отличается от метода Нестерова.
Также видно, что \textit{choose\_best} оказывает критическое влияние на
сходимость.

\subsection{Ускорение метода Ньютона}

Важным отличием представленных в проекте методов ускорения является их
применимость практически к любым методам, основанным на принципе неподвижной
точки. Для демонстрации этого свойства оптимизируем функцию
$$
f(x)=\log(e^x+e^{-x})
$$
с помощью метода Ньютона и его ускоренных версий. В данном случае методы
помогают достичь области квадратичной сходимости сразу после накопления
последовательности $\{x_i\}$ достаточной длины, чуть более медленная сходимость
ARNA объясняется более удачным попаданием в эту область, ибо в дальнейшем,
как видно в сравнении с обычным методом Ньютона, ускорение не участвует
в сходимости в силу опции \textit{choose\_best}.
\newpage

\begin{figure}[H]
  \includegraphics[width=\linewidth]{newton.png}
\end{figure}

\section{Список литературы}

\begin{enumerate}[{[}1{]}]
	\item Scieur, D., d’Aspremont, A., and Bach, F. (2016). "Regularized Nonlinear
	Acceleration". ArXiv e-prints, arXiv:1606.04133.
	\item \label{Nesterov} Nesterov, Y. (2013). "Introductory lectures on convex optimization: A basic course". Vol. 87, Springer Science \& Business Media.
\end{enumerate}

\end{document}